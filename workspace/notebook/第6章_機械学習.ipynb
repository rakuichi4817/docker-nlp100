{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aed4e2e-f188-4ca0-989a-7bbf1468d613",
   "metadata": {},
   "source": [
    "# 第6章: 機械学習\n",
    "\n",
    "> 本章では，Fabio Gasparetti氏が公開しているNews Aggregator Data Setを用い，  \n",
    "> ニュース記事の見出しを「ビジネス」「科学技術」「エンターテイメント」「健康」の  \n",
    "> カテゴリに分類するタスク（カテゴリ分類）に取り組む．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcf48f5-3b10-4a16-9d5b-d256289c522a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import random\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, confusion_matrix, precision_score, \n",
    "                             recall_score, f1_score, precision_recall_fscore_support, \n",
    "                             classification_report)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f29dd6d-14e3-4b90-ad15-af4ecb4be6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = \"data\"          # データを保存するおおもとのディレクトリ\n",
    "CURRENTDIR = \"/workspace\" # notebookを置いているディレクトリ\n",
    "\n",
    "# シード値の固定\n",
    "SEED = 19950908\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cf4a8c-bad2-4a4e-b2a3-d2c6092d9bca",
   "metadata": {},
   "source": [
    "## 50. データの入手・整形\n",
    "\n",
    "> News Aggregator Data Setをダウンロードし、以下の要領で学習データ（train.txt），検証データ（valid.txt），評価データ（test.txt）を作成せよ．\n",
    "> 1. ダウンロードしたzipファイルを解凍し，readme.txtの説明を読む．>\n",
    "> 2. 情報源（publisher）が”Reuters”, “Huffington Post”, “Businessweek”, “Contactmusic.com”, “Daily Mail”の事例（記事）のみを抽出する．\n",
    "> 3. 抽出された事例をランダムに並び替える．\n",
    "> 4. 抽出された事例の80%を学習データ，残りの10%ずつを検証データと評価データに分割し，それぞれtrain.txt，valid.txt，test.txtというファイル名で保存する．ファイルには，１行に１事例を書き出すこととし，カテゴリ名と記事見出しのタブ区切り形式とせよ（このファイルは後に問題70で再利用する）．\n",
    "> \n",
    "> 学習データと評価データを作成したら，各カテゴリの事例数を確認せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9184a5eb-39e7-4e63-bb60-77dc3b224f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6章で利用するデータを保管するディレクトリの作成\n",
    "CHAPDIR = os.path.join(DATADIR, \"chapter6\")\n",
    "try:\n",
    "    os.mkdir(CHAPDIR)\n",
    "except:\n",
    "    print(\"作成済み等の理由でディレクトリが作成されませんでした\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf4fe76-894d-46bf-8c3a-c3356343f8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ディレクトリの移動\n",
    "%cd $CURRENTDIR/$CHAPDIR\n",
    "\n",
    "# データのダウンロード\n",
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\n",
    "!unzip NewsAggregatorDataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da473c9-bc60-42ce-8c28-c962220185fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ディレクトリの移動\n",
    "%cd $CURRENTDIR/$CHAPDIR\n",
    "# 行数の確認\n",
    "!wc -l ./newsCorpora.csv\n",
    "# 先頭10行の確認\n",
    "!head -10 ./newsCorpora.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2268605-e39c-4d6f-8a9a-8ac9d509c046",
   "metadata": {},
   "source": [
    "---\n",
    "### データの抽出\n",
    "newsCorpora.csvのカラム情報（すべてタブ区切り。なんでcsvやねん）\n",
    "\n",
    "- ID\n",
    "- TITLE\n",
    "- URL\n",
    "- PUBLISHER\n",
    "- CATEGORY\n",
    "- STORY\n",
    "- HOSTNAME\n",
    "- TIMESTAMP\n",
    "\n",
    "\n",
    "以下のように指定されているので、抽出しながらデータを持つ\n",
    "> 情報源（publisher）が”Reuters”, “Huffington Post”, “Businessweek”, “Contactmusic.com”, “Daily Mail”の事例（記事）のみを抽出する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8e9a91-5e84-40e7-8ec6-93808eed2dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# カレントディレクトリをもどす\n",
    "%cd $CURRENTDIR\n",
    "\n",
    "# データの読み込み\n",
    "news_fpath = os.path.join(CHAPDIR, \"newsCorpora.csv\")\n",
    "# 抽出条件\n",
    "pub_filter = [\"Reuters\", \"Huffington Post\", \"Businessweek\", \"Contactmusic.com\", \"Daily Mail\"]\n",
    "# 利用する全データ\n",
    "all_data = []\n",
    "\n",
    "with open(news_fpath, \"r\", encoding=\"utf8\")as fr:\n",
    "    for line in fr:\n",
    "        # タブ区切りをリストで取得\n",
    "        record = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "        if record[3] in pub_filter:\n",
    "            #条件に当てはまるなら追加\n",
    "            all_data.append(record)\n",
    "\n",
    "print(\"使用するデータ数：\", len(all_data))\n",
    "pprint(all_data[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dd9986-0d1d-48f4-96ea-54572cc6b188",
   "metadata": {},
   "source": [
    "### 学習、検証、訓練データに分割\n",
    "\n",
    "以下の処理を行う\n",
    "\n",
    "> 抽出された事例をランダムに並び替える．\n",
    "> \n",
    "> 抽出された事例の80%を学習データ，残りの10%ずつを検証データと評価データに分割し，  \n",
    "> それぞれtrain.txt，valid.txt，test.txtというファイル名で保存する．  \n",
    "> ファイルには，１行に１事例を書き出すこととし，  \n",
    "> カテゴリ名と記事見出しのタブ区切り形式とせよ（このファイルは後に問題70で再利用する）．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85a3c7b-47f8-4b9b-9f56-1733f99ae4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習データやらの分割\n",
    "# 8:1:1で分割\n",
    "train_data, val_test_data = train_test_split(all_data, test_size=0.2, shuffle=True, \n",
    "                                             random_state=SEED, stratify=[r[4] for r in all_data])\n",
    "val_data, test_data = train_test_split(val_test_data, test_size=0.5, shuffle=True, \n",
    "                                       random_state=SEED, stratify=[r[4] for r in val_test_data])\n",
    "# 確認\n",
    "print(len(train_data), len(val_data), len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90def754-3bf8-4ed6-86bf-72ee24108345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの出力\n",
    "fname_lst = [\"train.txt\", \"valid.txt\", \"test.txt\"]\n",
    "split_data_lst = [train_data, val_data, test_data]\n",
    "\n",
    "for fname, data in zip(fname_lst, split_data_lst):\n",
    "    # 出力先のパス生成\n",
    "    output_fpath = os.path.join(CHAPDIR, fname)\n",
    "    with open(output_fpath, \"w\", encoding=\"utf8\")as fw:\n",
    "        for record in data:\n",
    "            # カテゴリとタイトルをタブ区切り\n",
    "            output_line = \"\\t\".join([record[4], record[1]])\n",
    "            fw.write(f\"{output_line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a184116d-d8ae-4639-8b39-8baaebbffe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 出力したデータの確認\n",
    "all_data_dct = {}\n",
    "\n",
    "for fname in fname_lst:\n",
    "    # 入力ファイル\n",
    "    input_fpath  =os.path.join(CHAPDIR, fname)\n",
    "    with open(input_fpath, \"r\", encoding=\"utf8\")as fr:\n",
    "        # タブ区切りをリストで取得\n",
    "        data = [line.rstrip(\"\\n\").split(\"\\t\") for line in fr]\n",
    "        all_data_dct[fname] = data\n",
    "\n",
    "# 各カテゴリの事例数を確認する\n",
    "for fname, data in all_data_dct.items():\n",
    "    print(\"【{}】\".format(fname))\n",
    "    pprint(collections.Counter([record[0] for record in data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3ac9d2-112e-46a1-911e-aeb05e70375e",
   "metadata": {},
   "source": [
    "## 51. 特徴量抽出\n",
    "\n",
    "> 学習データ，検証データ，評価データから特徴量を抽出し，それぞれtrain.feature.txt，valid.feature.txt，test.feature.txtというファイル名で保存せよ． なお，カテゴリ分類に有用そうな特徴量は各自で自由に設計せよ．記事の見出しを単語列に変換したものが最低限のベースラインとなるであろう．\n",
    "\n",
    "とりあえず英語のストップワードとTF-IDFでやる。\n",
    "\n",
    "前処理に関しては以下の内容に取り組む\n",
    "- 小文字化\n",
    "- 「's」「...」「-」の削除\n",
    "- 数字の0化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0401b1e9-3bc8-4503-8032-556bd8a90518",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 前処理用の正規表現\n",
    "re_del = re.compile(\"('s|-|\\.\\.\\.)\")\n",
    "re_num = re.compile(r\"[０-９0-9]([０-９．,0-9.,]?[０-９0-9])*\")\n",
    "\n",
    "# 前処理関数を作成\n",
    "def preprocessing(text):\n",
    "    text = text.lower()          # 小文字化\n",
    "    text = re_del.sub(\" \", text) # 特定文字の削除 \n",
    "    text = re_num.sub(\"0\", text) # 数字の0化\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641dbeb5-da44-4b88-a88d-6e01ae2f473f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全データに前処理の適応\n",
    "\n",
    "# 訓練データ\n",
    "train_title_lst = []\n",
    "y_train = []\n",
    "for record in all_data_dct[\"train.txt\"]:\n",
    "    train_title_lst.append(preprocessing(record[1]))\n",
    "    y_train.append(record[0])\n",
    "\n",
    "# 検証データ\n",
    "val_title_lst = []\n",
    "y_val = []\n",
    "for record in all_data_dct[\"valid.txt\"]:\n",
    "    val_title_lst.append(preprocessing(record[1]))\n",
    "    y_val.append(record[0])\n",
    "\n",
    "# 試験データ\n",
    "test_title_lst = []\n",
    "y_test = []\n",
    "for record in all_data_dct[\"test.txt\"]:\n",
    "    test_title_lst.append(preprocessing(record[1]))\n",
    "    y_test.append(record[0])            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cffb31f-4f28-4d9d-9efd-45581b02be3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ベクタライザーのインスタンス化\n",
    "vectorizer = TfidfVectorizer(min_df=5, ngram_range=(1, 2), stop_words=\"english\")\n",
    "\n",
    "# 訓練+検証データでTF-IDFベクトルの作成\n",
    "X_train_val = vectorizer.fit_transform(train_title_lst+val_title_lst)\n",
    "# 試験データに適応\n",
    "X_test = vectorizer.transform(test_title_lst)\n",
    "\n",
    "# 訓練データと検証データの再分割\n",
    "X_train = X_train_val[:len(train_title_lst)]\n",
    "X_val = X_train_val[len(train_title_lst):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3654ec8-e441-404d-96c8-a678afdafe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファイルへの出力\n",
    "fname_lst = [\"train.feature.txt\", \"valid.feature.txt\", \"test.feature.txt\"]\n",
    "split_X_lst = [X_train, X_val, X_test]\n",
    "\n",
    "for fname, X_data in zip(fname_lst, split_X_lst):\n",
    "    # 出力するファイルパス\n",
    "    output_fpath = os.path.join(CHAPDIR, fname)\n",
    "    # sparseデータをCSVにするのにpandasが実装上手っ取り早いので\n",
    "    df_temp = pd.DataFrame(X_data.toarray(), columns=vectorizer.get_feature_names())\n",
    "    # 出力\n",
    "    df_temp.to_csv(output_fpath, sep='\\t', index=False)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae520a9-0782-4c5b-8d69-a060bcf02b0a",
   "metadata": {},
   "source": [
    "## 52. 学習\n",
    "\n",
    "> 51で構築した学習データを用いて，ロジスティック回帰モデルを学習せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4a3585-eaba-4a43-8b20-2801f4fd6851",
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_model = LogisticRegression(random_state=SEED, max_iter=10000)\n",
    "lg_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c37c7f-cc71-4572-8c89-bfe154a4218b",
   "metadata": {},
   "source": [
    "## 53. 予測\n",
    "\n",
    "> 52で学習したロジスティック回帰モデルを用い，与えられた記事見出しからカテゴリとその予測確率を計算するプログラムを実装せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576d39b6-49ed-403b-b316-b743917acf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 試し\n",
    "lg_model.predict_proba(X_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7196d819-cac7-4dde-a196-1d16e2bb5fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predict_score(lg_model, X):\n",
    "    return (np.max(lg_model.predict_proba(X), axis=1), lg_model.predict(X))\n",
    "\n",
    "get_predict_score(lg_model, X_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80a999b-eac4-47b3-9fd4-01e07756a785",
   "metadata": {},
   "source": [
    "## 54. 正解率の計測\n",
    "\n",
    "> 52で学習したロジスティック回帰モデルの正解率を，学習データおよび評価データ上で計測せよ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a84255-d5c0-4156-bb89-a6af2b9edfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測値の取得\n",
    "pred_train = get_predict_score(lg_model, X_train)\n",
    "pred_test = get_predict_score(lg_model, X_test)\n",
    "\n",
    "acc_train = accuracy_score(y_train, pred_train[1])\n",
    "acc_test = accuracy_score(y_test, pred_test[1])\n",
    "\n",
    "print(f\"accuracy（train）：{acc_train:.3f}\")\n",
    "print(f'accuracy（test）：{acc_test:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24630cdc-7a6d-464b-8b57-54bebc26e801",
   "metadata": {},
   "source": [
    "## 55. 混同行列の作成\n",
    "\n",
    "> 52で学習したロジスティック回帰モデルの混同行列（confusion matrix）を，学習データおよび評価データ上で作成せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67909ba-3428-4523-bc0d-23f65fda900a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習データ\n",
    "train_conf_mat = confusion_matrix(y_train, pred_train[1])\n",
    "sns.heatmap(train_conf_mat, annot=True, cmap=\"Blues\")\n",
    "plt.show()\n",
    "\n",
    "# 評価データ\n",
    "test_conf_mat = confusion_matrix(y_test, pred_test[1])\n",
    "sns.heatmap(test_conf_mat, annot=True, cmap=\"Blues\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18ccd11-6be1-46e2-b79e-a401dd1e4c7d",
   "metadata": {},
   "source": [
    "## 56. 適合率，再現率，F1スコアの計測\n",
    "\n",
    "> 52で学習したロジスティック回帰モデルの適合率，再現率，F1スコアを，評価データ上で計測せよ．  \n",
    "> カテゴリごとに適合率，再現率，F1スコアを求め，カテゴリごとの性能をマイクロ平均（micro-average）とマクロ平均（macro-average）で統合せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab86646-697d-454a-a457-921481c2e1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# micro avgとaccuracyが基本一致する\n",
    "print(classification_report(y_test, pred_test[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797d77dc-335c-493f-902b-be4d97ec09d5",
   "metadata": {},
   "source": [
    "## 57. 特徴量の重みの確認\n",
    "\n",
    "> 52で学習したロジスティック回帰モデルの中で，重みの高い特徴量トップ10と，重みの低い特徴量トップ10を確認せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70480aa3-83f9-4c2e-b8ab-344cb4265abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴次元の取得\n",
    "vocabs = vectorizer.get_feature_names()\n",
    "# 重要度の取得\n",
    "for c, coefs in zip(lg_model.classes_, lg_model.coef_):\n",
    "    print(\"----- カテゴリ：{} -----\".format(c))\n",
    "    # 重要度が高い10件\n",
    "    top10_idxs = np.argsort(coefs)[::-1][:10]\n",
    "    print(\"重要度上位：\", \", \".join([vocabs[idx] for idx in top10_idxs]))\n",
    "    # 重要度が低い10件\n",
    "    bottom10_idxs = np.argsort(coefs)[:10]\n",
    "    print(\"重要度下位：\", \", \".join([vocabs[idx] for idx in bottom10_idxs]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fae4f2-cc47-4e51-bef3-ae98c3d54ce7",
   "metadata": {},
   "source": [
    "## 58. 正則化パラメータの変更\n",
    "\n",
    "ここはQiitaの回答丸パクリ\n",
    "\n",
    "> ロジスティック回帰モデルを学習するとき，正則化パラメータを調整することで，  \n",
    "> 学習時の過学習（overfitting）の度合いを制御できる．  \n",
    "> 異なる正則化パラメータでロジスティック回帰モデルを学習し，学習データ，検証データ，および評価データ上の正解率を求めよ．  \n",
    "> 実験の結果は，正則化パラメータを横軸，正解率を縦軸としたグラフにまとめよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e747a1da-44df-4d2e-aad8-7c3f3d7df2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 出力用リスト\n",
    "results = []\n",
    "for C in tqdm(np.logspace(-5, 4, 10, base=10)):\n",
    "    # 学習\n",
    "    lg_model = LogisticRegression(random_state=SEED, max_iter=10000, C=C)\n",
    "    lg_model.fit(X_train, y_train)\n",
    "    \n",
    "    # 予測\n",
    "    pred_train = get_predict_score(lg_model, X_train)\n",
    "    pred_val = get_predict_score(lg_model, X_val)\n",
    "    pred_test = get_predict_score(lg_model, X_test)\n",
    "    \n",
    "    # accuracyを求める\n",
    "    acc_train = accuracy_score(y_train, pred_train[1])\n",
    "    acc_val = accuracy_score(y_val, pred_val[1])\n",
    "    acc_test = accuracy_score(y_test, pred_test[1])\n",
    "    \n",
    "    # 結果の格納\n",
    "    results.append([C, acc_train, acc_val, acc_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fc6b04-fa86-49e8-b289-ced6b64b0450",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b61e75-64c4-4aaf-b97f-71b64bbaac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可視化\n",
    "results = np.array(results).T\n",
    "plt.plot(results[0], results[1], label='train')\n",
    "plt.plot(results[0], results[2], label='valid')\n",
    "plt.plot(results[0], results[3], label='test')\n",
    "plt.ylim(0, 1.1)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xscale ('log')\n",
    "plt.xlabel('C')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b046f5ef-0ccc-4e9a-8d88-8342a2e8b359",
   "metadata": {},
   "source": [
    "## 59. ハイパーパラメータの探索\n",
    "\n",
    "> 学習アルゴリズムや学習パラメータを変えながら，カテゴリ分類モデルを学習せよ．検証データ上の正解率が最も高くなる学習アルゴリズム・パラメータを求めよ．また，その学習アルゴリズム・パラメータを用いたときの評価データ上の正解率を求めよ．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f1fd07-264f-4611-90ee-405ebe7c2483",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
