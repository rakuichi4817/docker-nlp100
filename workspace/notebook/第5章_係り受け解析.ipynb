{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第5章: 係り受け解析\n",
    "\n",
    "> 日本語Wikipediaの「人工知能」に関する記事からテキスト部分を抜き出したファイルが[ai.ja.zip](https://nlp100.github.io/data/ai.ja.zip)に収録されている． この文章をCaboChaやKNP等のツールを利用して係り受け解析を行い，その結果をai.ja.txt.parsedというファイルに保存せよ．このファイルを読み込み，以下の問に対応するプログラムを実装せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import pydot\n",
    "import japanize_matplotlib\n",
    "from graphviz import Digraph\n",
    "from IPython.display import Image,display_png\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データを保存するディレクトリの作成\n",
    "DATADIR = \"data\"\n",
    "CURRENTDIR = \"/workspace/notebook\"\n",
    "CHAPDIR = os.path.join(DATADIR, \"chapter5\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(CHAPDIR)\n",
    "except:\n",
    "    print(\"作成済み等の理由でディレクトリが作成されませんでした\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"ai.ja.txt.parsed\"\n",
    "in_fpath = os.path.join(CHAPDIR, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 326,
     "status": "ok",
     "timestamp": 1622161065155,
     "user": {
      "displayName": "keisuke",
      "photoUrl": "",
      "userId": "02662014936449507155"
     },
     "user_tz": -540
    },
    "id": "AUzsrGSzqbGS"
   },
   "outputs": [],
   "source": [
    "%cd $CURRENTDIR/$CHAPDIR/\n",
    "!wget https://nlp100.github.io/data/ai.ja.zip\n",
    "!unzip ai.ja.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2622,
     "status": "ok",
     "timestamp": 1622161068723,
     "user": {
      "displayName": "keisuke",
      "photoUrl": "",
      "userId": "02662014936449507155"
     },
     "user_tz": -540
    },
    "id": "TdfBYlAF7m1J"
   },
   "outputs": [],
   "source": [
    "!cabocha -f1 -o ai.ja.txt.parsed ai.ja.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1622185579785,
     "user": {
      "displayName": "keisuke",
      "photoUrl": "",
      "userId": "02662014936449507155"
     },
     "user_tz": -540
    },
    "id": "lbGHhUcAOB5g",
    "outputId": "0551128c-038b-4257-807b-a111e58a1282"
   },
   "outputs": [],
   "source": [
    "# 確認\n",
    "!head -10 ./ai.ja.txt.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd $CURRENTDIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gqmONWud0fw4"
   },
   "source": [
    "## 40. 係り受け解析結果の読み込み（形態素）  \n",
    "\n",
    "> 形態素を表すクラスMorphを実装せよ．このクラスは表層形（surface），基本形（base），品詞（pos），品詞細分類1（pos1）をメンバ変数に持つこととする．さらに，係り受け解析の結果（ai.ja.txt.parsed）を読み込み，各文をMorphオブジェクトのリストとして表現し，冒頭の説明文の形態素列を表示せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 343,
     "status": "ok",
     "timestamp": 1622185980918,
     "user": {
      "displayName": "keisuke",
      "photoUrl": "",
      "userId": "02662014936449507155"
     },
     "user_tz": -540
    },
    "id": "3BiJeofVOEMG"
   },
   "outputs": [],
   "source": [
    "class Morph:\n",
    "    def __init__(self, morph):\n",
    "        # タブで二つに区切られている\n",
    "        fields = morph.split(\"\\t\")\n",
    "        self.surface = fields[0]\n",
    "        # タブで区切られた2つ目の要素はカンマ区切り\n",
    "        attr = fields[1].split(\",\")\n",
    "        self.base = attr[6]  \n",
    "        self.pos = attr[0]  \n",
    "        self.pos1 = attr[1]  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences = list()\n",
    "morphs = list()\n",
    "\n",
    "with open(in_fpath, \"r\", encoding=\"utf8\")as fr:\n",
    "    for line in fr:\n",
    "        if (line != \"EOS\\n\") and (line[0] != '*'):\n",
    "            morphs.append(Morph(line))\n",
    "        elif line == \"EOS\\n\":\n",
    "            sentences.append(morphs)\n",
    "            morphs = []\n",
    "\n",
    "for m in sentences[2]:\n",
    "    print(vars(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxUH8ZKL_ZmM"
   },
   "source": [
    "## 41. 係り受け解析結果の読み込み（文節・係り受け）  \n",
    "\n",
    "> 40に加えて，文節を表すクラスChunkを実装せよ．このクラスは形態素（Morphオブジェクト）のリスト（morphs），係り先文節インデックス番号（dst），係り元文節インデックス番号のリスト（srcs）をメンバ変数に持つこととする．さらに，入力テキストの係り受け解析結果を読み込み，１文をChunkオブジェクトのリストとして表現し，冒頭の説明文の文節の文字列と係り先を表示せよ．本章の残りの問題では，ここで作ったプログラムを活用せよ．\n",
    "\n",
    "\n",
    "CaboChaの出力\n",
    "\n",
    "```text\n",
    "* 0 -1D 1/1 0.000000\n",
    "人工\t名詞,一般,*,*,*,*,人工,ジンコウ,ジンコー\n",
    "知能\t名詞,一般,*,*,*,*,知能,チノウ,チノー\n",
    "EOS\n",
    "EOS\n",
    "* 0 17D 1/1 0.388993\n",
    "人工\t名詞,一般,*,*,*,*,人工,ジンコウ,ジンコー\n",
    "知能\t名詞,一般,*,*,*,*,知能,チノウ,チノー\n",
    "* 1 17D 2/3 0.613549\n",
    "（\t記号,括弧開,*,*,*,*,（,（,（* 0 -1D 1/1 0.000000\n",
    "人工\t名詞,一般,*,*,*,*,人工,ジンコウ,ジンコー\n",
    "知能\t名詞,一般,*,*,*,*,知能,チノウ,チノー\n",
    "EOS\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chunk:\n",
    "    def __init__(self, morphs, dst):\n",
    "        self.morphs = morphs  # 形態素（Morphオブジェクト）のリスト\n",
    "        self.dst = dst        # 係り先文節インデックス番号\n",
    "        self.srcs = []        # 係り元文節インデックス番号のリスト "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences = [] # 各文の情報を追加\n",
    "chunks = [] # 各文節リスト\n",
    "morphs = [] # 各文節の形態素リスト\n",
    "src_idxs = defaultdict(list) # 「係り先idx: [係り元idxs（リスト）]」の辞書\n",
    "\n",
    "with open(in_fpath, \"r\", encoding=\"utf8\")as fr:    \n",
    "    for line in fr:\n",
    "        # 文節情報 or 形態素情報 or EOS で場合分け\n",
    "        if line.startswith(\"*\"):\n",
    "            # 文節情報の時\n",
    "            if morphs:\n",
    "                chunks.append(Chunk(morphs, dst))\n",
    "                morphs = [] # 次の文節なので初期化\n",
    "            \n",
    "            chunk_info = line.rstrip(\"\\n\").split(\" \")\n",
    "            chunk_idx = int(chunk_info[1])\n",
    "            dst = int(chunk_info[2].rstrip(\"D\"))\n",
    "            if dst != -1:\n",
    "                src_idxs[dst].append(chunk_idx)\n",
    "        elif not line.startswith(\"EOS\\n\"):\n",
    "            # 形態素情報の時\n",
    "            morphs.append(Morph(line))\n",
    "        else:\n",
    "            # EOSのとき\n",
    "            # 空行もあるので飛ばす\n",
    "            if morphs:\n",
    "                chunks.append(Chunk(morphs, dst))\n",
    "                if src_idxs:\n",
    "                    for chunk_idx, srcs in src_idxs.items():\n",
    "                        chunks[chunk_idx].srcs = srcs\n",
    "                if chunks:\n",
    "                    sentences.append(chunks)\n",
    "            # 初期化\n",
    "            morphs = [] \n",
    "            chunks = []\n",
    "            src_idxs = defaultdict(list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 確認\n",
    "for i, chunk in enumerate(sentences[1]):\n",
    "    print(i, vars(chunk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IhQIDt3EdJbK"
   },
   "source": [
    "## 42. 係り元と係り先の文節の表示  \n",
    "\n",
    "> 係り元の文節と係り先の文節のテキストをタブ区切り形式ですべて抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1622185981765,
     "user": {
      "displayName": "keisuke",
      "photoUrl": "",
      "userId": "02662014936449507155"
     },
     "user_tz": -540
    },
    "id": "u6R-bNZlVubK",
    "outputId": "7c4bc220-c9ef-4a86-d1ff-e49f531902e0"
   },
   "outputs": [],
   "source": [
    "def get_pair(chunks):\n",
    "    pairs = []\n",
    "    for chunk in chunks:\n",
    "        dst = chunk.dst\n",
    "        if dst!= -1:\n",
    "            # 係り元文節\n",
    "            modifier = \"\".join([morph.surface if morph.pos != \"記号\" else \"\" for morph in chunk.morphs])\n",
    "            # 係り先文節\n",
    "            modifiee = \"\".join([morph.surface if morph.pos != \"記号\" else \"\" for morph in chunks[dst].morphs])\n",
    "            pairs.append([modifier, modifiee])\n",
    "    return pairs\n",
    "    \n",
    "for pair in get_pair(sentences[1]):\n",
    "    print(\"\\t\".join(pair))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_b_lMz2jL6J"
   },
   "source": [
    "## 43. 名詞を含む文節が動詞を含む文節に係るものを抽出  \n",
    "\n",
    "> 名詞を含む文節が，動詞を含む文節に係るとき，これらをタブ区切り形式で抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1622185981767,
     "user": {
      "displayName": "keisuke",
      "photoUrl": "",
      "userId": "02662014936449507155"
     },
     "user_tz": -540
    },
    "id": "9v6fRhQieZwr",
    "outputId": "abe26e47-b153-4747-d326-7bca53cac094"
   },
   "outputs": [],
   "source": [
    "def get_norn_verb_pair(chunks):\n",
    "    pairs = []\n",
    "    for chunk in chunks:\n",
    "        dst = chunk.dst\n",
    "        if dst!= -1:\n",
    "            # 係り元文節\n",
    "            # 名詞を含むかの確認\n",
    "            if \"名詞\" in [morph.pos for morph in chunk.morphs]:\n",
    "                modifier = \"\".join([morph.surface if morph.pos != \"記号\" else \"\" for morph in chunk.morphs])\n",
    "                # 係り先文節\n",
    "                # 動詞を含むかの確認\n",
    "                if \"動詞\" in [morph.pos for morph in chunks[dst].morphs]:\n",
    "                    modifiee = \"\".join([morph.surface if morph.pos != \"記号\" else \"\" for morph in chunks[dst].morphs])\n",
    "                    pairs.append([modifier, modifiee])\n",
    "    return pairs\n",
    "\n",
    "\n",
    "for pair in get_norn_verb_pair(sentences[1]):\n",
    "    print(\"\\t\".join(pair))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Q8xpOlTCwdI"
   },
   "source": [
    "## 44. 係り受け木の可視化  \n",
    "\n",
    "> 与えられた文の係り受け木を有向グラフとして可視化せよ．可視化には，Graphviz等を用いるとよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edges(chunks):\n",
    "    edges = []\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        dst = chunk.dst\n",
    "        if dst!= -1:\n",
    "            modifier = \"\".join([morph.surface if morph.pos != \"記号\" else \"\" for morph in chunk.morphs])\n",
    "            modifier_idx = f\"({idx})\"\n",
    "            modifiee = \"\".join([morph.surface if morph.pos != \"記号\" else \"\" for morph in chunks[dst].morphs])\n",
    "            modifiee_idx = f\"({dst})\"\n",
    "            edges.append([f\"{modifier}{modifier_idx}\", f\"{modifiee}{modifiee_idx}\"])\n",
    "    return edges\n",
    "\n",
    "# グラフ保存ファイル\n",
    "img_path = os.path.join(CHAPDIR, \"ans44.png\")\n",
    "\n",
    "edges = get_edges(sentences[1])\n",
    "g = pydot.graph_from_edges(edges, directed=True)\n",
    "g.write_png(img_path)\n",
    "display_png(Image(img_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWjd3FVKnzZ9"
   },
   "source": [
    "## 45. 動詞の格パターンの抽出  \n",
    "\n",
    "> 今回用いている文章をコーパスと見なし，日本語の述語が取りうる格を調査したい． 動詞を述語，動詞に係っている文節の助詞を格と考え，述語と格をタブ区切り形式で出力せよ． ただし，出力は以下の仕様を満たすようにせよ．\n",
    "> * 動詞を含む文節において，最左の動詞の基本形を述語とする\n",
    "> * 述語に係る助詞を格とする\n",
    "> * 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    ">\n",
    "> 「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える． この文は「作り出す」という１つの動詞を含み，「作り出す」に係る文節は「ジョン・マッカーシーは」，「会議で」，「用語を」であると解析された場合は，次のような出力になるはずである．\n",
    "> >作り出す\tで は を\n",
    ">\n",
    "> このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．\n",
    "> * コーパス中で頻出する述語と格パターンの組み合わせ\n",
    "> * 「行う」「なる」「与える」という動詞の格パターン（コーパス中で出現頻度の高い順に並べよ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 出力先\n",
    "out_fpath = os.path.join(CHAPDIR, \"ans45.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_verb_kaku(chunks):\n",
    "    outputs = []\n",
    "    for chunk in chunks:\n",
    "        for morph in chunk.morphs:\n",
    "            # 文節の左側の形態素から動詞チェック\n",
    "            if morph.pos == \"動詞\":\n",
    "                # 係り元の文節インデックスの確認\n",
    "                kaku_patterns = []\n",
    "                for src in chunk.srcs:\n",
    "                    kaku_patterns.extend([morph.surface for morph in chunks[src].morphs if morph.pos == \"助詞\"])\n",
    "                if kaku_patterns:\n",
    "                    kaku_patterns = sorted(list(set(kaku_patterns)))\n",
    "                    output_kaku_pattern = ' '.join(kaku_patterns)\n",
    "                    outputs.append(f\"{morph.base}\\t{output_kaku_pattern}\")\n",
    "                    # 一番左の動詞とそれにかかる格助詞のみ取得するので\n",
    "                    break\n",
    "    return outputs\n",
    "\n",
    "with open(out_fpath, \"w\", encoding=\"utf8\") as fw:\n",
    "    for sentence in sentences:\n",
    "        if sentence:\n",
    "            output_lines = get_verb_kaku(sentence)\n",
    "            if output_lines:\n",
    "                fw.write(\"\\n\".join(output_lines) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 446,
     "status": "ok",
     "timestamp": 1622185985055,
     "user": {
      "displayName": "keisuke",
      "photoUrl": "",
      "userId": "02662014936449507155"
     },
     "user_tz": -540
    },
    "id": "LgcA7KhqqCoB",
    "outputId": "6623181f-a2b8-48ce-ffe9-91911e05e993"
   },
   "outputs": [],
   "source": [
    "# 確認\n",
    "!cat $out_fpath | sort | uniq -c | sort -nr | head -n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 70,
     "status": "ok",
     "timestamp": 1622185985057,
     "user": {
      "displayName": "keisuke",
      "photoUrl": "",
      "userId": "02662014936449507155"
     },
     "user_tz": -540
    },
    "id": "WSMZszS58go2",
    "outputId": "97b503ee-34fe-4783-876e-e7725707d6bd"
   },
   "outputs": [],
   "source": [
    "!cat $out_fpath | grep '行う' | sort | uniq -c | sort -nr | head -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 67,
     "status": "ok",
     "timestamp": 1622185985059,
     "user": {
      "displayName": "keisuke",
      "photoUrl": "",
      "userId": "02662014936449507155"
     },
     "user_tz": -540
    },
    "id": "bY1R1DUf8kXS",
    "outputId": "bbe88231-df34-476d-dc10-24e6a7afbfc0"
   },
   "outputs": [],
   "source": [
    "!cat $out_fpath | grep 'なる' | sort | uniq -c | sort -nr | head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83jJ_kuj4hIp"
   },
   "source": [
    "## 46. 動詞の格フレーム情報の抽出\n",
    "\n",
    "> 45のプログラムを改変し，述語と格パターンに続けて項（述語に係っている文節そのもの）をタブ区切り形式で出力せよ．45の仕様に加えて，以下の仕様を満たすようにせよ．\n",
    "> * 項は述語に係っている文節の単語列とする（末尾の助詞を取り除く必要はない）\n",
    "> * 述語に係る文節が複数あるときは，助詞と同一の基準・順序でスペース区切りで並べる\n",
    "> \n",
    "> 「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える． この文は「作り出す」という１つの動詞を含み，「作り出す」に係る文節は「ジョン・マッカーシーは」，「会議で」，「用語を」であると解析された場合は，次のような出力になるはずである．\n",
    "> \n",
    "> ```作り出す\tで は を\t会議で ジョンマッカーシーは 用語を```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### コメント\n",
    "\n",
    "出力ルールが不明瞭。「助詞と同一の基準」の解釈が、  \n",
    "「助詞と同じ順序に並び替えろ」なのか、「助詞と同じように辞書順に並び替えろ」なのかがわかりづらい。  \n",
    "本回答では後者の解釈で実装している。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 出力先\n",
    "out_fpath = os.path.join(CHAPDIR, \"ans46.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_verb_kaku2(chunks):\n",
    "    outputs = []\n",
    "    for chunk in chunks:\n",
    "        for morph in chunk.morphs:\n",
    "            # 文節の左側の形態素から動詞チェック\n",
    "            if morph.pos == \"動詞\":\n",
    "                # 係り元の文節インデックスの確認\n",
    "                kaku_patterns = []\n",
    "                modifier = []\n",
    "                for src in chunk.srcs:\n",
    "                    kaku_patterns.extend([morph.surface for morph in chunks[src].morphs if morph.pos == \"助詞\"])\n",
    "                    modifier.append(\"\".join([morph.surface for morph in chunks[src].morphs if morph.pos != \"記号\"]))\n",
    "                if kaku_patterns:\n",
    "                    kaku_patterns = sorted(list(set(kaku_patterns)))\n",
    "                    modifier = sorted(list(set(modifier)))\n",
    "                    output_kaku_pattern = ' '.join(kaku_patterns)\n",
    "                    output_modifier = ' '.join(modifier)\n",
    "                    outputs.append(f\"{morph.base}\\t{output_kaku_pattern}\\t{output_modifier}\")\n",
    "                    # 一番左の動詞とそれにかかる格助詞のみ取得するので\n",
    "                    break\n",
    "    return outputs\n",
    "\n",
    "with open(out_fpath, \"w\", encoding=\"utf8\") as fw:\n",
    "    for sentence in sentences:\n",
    "        if sentence:\n",
    "            output_lines = get_verb_kaku2(sentence)\n",
    "            if output_lines:\n",
    "                fw.write(\"\\n\".join(output_lines) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1622185985069,
     "user": {
      "displayName": "keisuke",
      "photoUrl": "",
      "userId": "02662014936449507155"
     },
     "user_tz": -540
    },
    "id": "ub4RsWUY8ZBw",
    "outputId": "7987ccf4-bc42-495e-df8e-f2b039377568"
   },
   "outputs": [],
   "source": [
    "# 確認\n",
    "!cat $out_fpath| head -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XclwX_3K-3cs"
   },
   "source": [
    "## 47. 機能動詞構文のマイニング\n",
    "\n",
    "> 動詞のヲ格にサ変接続名詞が入っている場合のみに着目したい．46のプログラムを以下の仕様を満たすように改変せよ．\n",
    "> * 「サ変接続名詞+を（助詞）」で構成される文節が動詞に係る場合のみを対象とする\n",
    "> * 述語は「サ変接続名詞+を+動詞の基本形」とし，文節中に複数の動詞があるときは，最左の動詞を用いる\n",
    "> * 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    "> * 述語に係る文節が複数ある場合は，すべての項をスペース区切りで並べる（助詞の並び順と揃えよ）\n",
    "> \n",
    "> 例えば「また、自らの経験を元に学習を行う強化学習という手法もある。」という文から，以下の出力が得られるはずである．\n",
    "> >学習を行う\tに を\t元に 経験を"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### コメント\n",
    "\n",
    "- 述語にかかるという解釈で、述語が「サ変接続名詞+を+動詞の基本形」なのであれば、  それぞれの文節かかる文節すべてが必要になる\n",
    "- 複数のサ変接続名詞が文節内に存在していることを考慮していない\n",
    "\n",
    "\n",
    "**下の回答例は以下の特徴を持つ**\n",
    "\n",
    "- 述語にかかる助詞と文節の並び順がおかしい\n",
    "    - 係り元の文節に複数の助詞が含まれる場合、順序が崩れてしまう\n",
    "- 複数のサ変接続名詞を持つ文節には対応している"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 出力先\n",
    "out_fpath = os.path.join(CHAPDIR, \"ans47.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_verb_wokaku(chunks):\n",
    "    outputs = []\n",
    "    for chunk in chunks:\n",
    "        for morph in chunk.morphs:\n",
    "            # 文節の左側の形態素から動詞チェック\n",
    "            if morph.pos == \"動詞\":\n",
    "                # 係り元の文節インデックスの確認\n",
    "                kaku_patterns = []\n",
    "                modifier = []\n",
    "                predicate = None\n",
    "                for src in chunk.srcs:\n",
    "                    if \"サ変接続\" in [morph.pos1 for morph in chunks[src].morphs] and \\\n",
    "                        \"を\" in [morph.surface for morph in chunks[src].morphs]:\n",
    "                        predicate = \"\".join([morph.surface for morph in chunks[src].morphs if morph.pos1 == \"サ変接続\" or morph.surface == \"を\"])\n",
    "                        predicate = f\"{predicate}{morph.base}\"\n",
    "                    else:\n",
    "                        kaku_patterns.extend([morph.surface for morph in chunks[src].morphs if morph.pos == \"助詞\"])\n",
    "                        modifier.append(\"\".join([morph.surface for morph in chunks[src].morphs if morph.pos != \"記号\"]))\n",
    "                if predicate is not None:\n",
    "                    output = f\"{predicate}\"\n",
    "                    if kaku_patterns:\n",
    "                        kaku_patterns = list(set(kaku_patterns))\n",
    "                        modifier = list(set(modifier))\n",
    "                        kaku_patterns, modifier = zip(*sorted(zip(kaku_patterns, modifier)))\n",
    "                        output_kaku_pattern = ' '.join(kaku_patterns)\n",
    "                        output_modifier = ' '.join(modifier)\n",
    "                        output = f\"{predicate}\\t{output_kaku_pattern}\\t{output_modifier}\"\n",
    "                    outputs.append(output)\n",
    "                    # 一番左の動詞とそれにかかる格助詞のみ取得するので\n",
    "                    break\n",
    "\n",
    "    return outputs\n",
    "\n",
    "with open(out_fpath, \"w\", encoding=\"utf8\") as fw:\n",
    "    for sentence in sentences:\n",
    "        if sentence:\n",
    "            output_lines = get_verb_wokaku(sentence)\n",
    "            if output_lines:\n",
    "                fw.write(\"\\n\".join(output_lines) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1622185985418,
     "user": {
      "displayName": "keisuke",
      "photoUrl": "",
      "userId": "02662014936449507155"
     },
     "user_tz": -540
    },
    "id": "6T4MlVjOVfSJ",
    "outputId": "6e144448-51b3-4872-996e-632d5487ce9b"
   },
   "outputs": [],
   "source": [
    "# 確認\n",
    "!cat $out_fpath | cut -f 1 | sort | uniq -c | sort -nr | head -n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1622185985419,
     "user": {
      "displayName": "keisuke",
      "photoUrl": "",
      "userId": "02662014936449507155"
     },
     "user_tz": -540
    },
    "id": "tesfS9HmW4th",
    "outputId": "55808dfe-bccc-454b-d735-f4d8fa84d0f1"
   },
   "outputs": [],
   "source": [
    "!cat $out_fpath | cut -f 1,2 | sort | uniq -c | sort -nr | head -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rbKUXYb-icC-"
   },
   "source": [
    "## 48. 名詞から根へのパスの抽出\n",
    "\n",
    "> 文中のすべての名詞を含む文節に対し，その文節から構文木の根に至るパスを抽出せよ． ただし，構文木上のパスは以下の仕様を満たすものとする．\n",
    "> * 各文節は（表層形の）形態素列で表現する\n",
    "> * パスの開始文節から終了文節に至るまで，各文節の表現を” -> “で連結する\n",
    "> \n",
    "> 「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える． CaboChaを係り受け解析に用いた場合，次のような出力が得られると思われる．\n",
    "> >ジョンマッカーシーは -> 作り出した  \n",
    "> >AIに関する -> 最初の -> 会議で -> 作り出した  \n",
    "> >最初の -> 会議で -> 作り出した  \n",
    "> >会議で -> 作り出した  \n",
    "> >人工知能という -> 用語を -> 作り出した  \n",
    "> >用語を -> 作り出した\n",
    "> \n",
    "> KNPを係り受け解析に用いた場合，次のような出力が得られると思われる．\n",
    "> >ジョンマッカーシーは -> 作り出した  \n",
    "> >ＡＩに -> 関する -> 会議で -> 作り出した  \n",
    "> >会議で -> 作り出した  \n",
    "> >人工知能と -> いう -> 用語を -> 作り出した  \n",
    "> >用語を -> 作り出した\n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 出力先\n",
    "out_fpath = os.path.join(CHAPDIR, \"ans48.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norn_deps(chunks):\n",
    "    outputs = []\n",
    "    for chunk in chunks:\n",
    "        # 名詞を含む文節なら\n",
    "        if \"名詞\" in [morph.pos for morph in chunk.morphs]:\n",
    "            path_chunks = [''.join([morph.surface for morph in chunk.morphs if morph.pos != \"記号\"])]\n",
    "            # そこから係り先をすべて見ていく\n",
    "            dst_chunk_idx = chunk.dst\n",
    "            while dst_chunk_idx != -1:\n",
    "                path_chunks.append(\"\".join([morph.surface for morph in chunks[dst_chunk_idx].morphs if morph.pos != \"記号\"]))\n",
    "                dst_chunk_idx = chunks[dst_chunk_idx].dst\n",
    "            if len(path_chunks) > 1:\n",
    "                outputs.append(\" -> \".join(path_chunks))\n",
    "    return outputs\n",
    "    \n",
    "\n",
    "with open(out_fpath, \"w\", encoding=\"utf8\") as fw:\n",
    "    for sentence in sentences:\n",
    "        if sentence:\n",
    "            output_lines = get_norn_deps(sentence)\n",
    "            if output_lines:\n",
    "                fw.write(\"\\n\".join(output_lines) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sHeitKy9slPi"
   },
   "source": [
    "## 49. 名詞間の係り受けパスの抽出\n",
    "\n",
    "> 文中のすべての名詞句のペアを結ぶ最短係り受けパスを抽出せよ．ただし，名詞句ペアの文節番号がiとj（i<j）のとき，係り受けパスは以下の仕様を満たすものとする．\n",
    "> * 問題48と同様に，パスは開始文節から終了文節に至るまでの各文節の表現（表層形の形態素列）を” -> “で連結して表現する\n",
    "> * 文節iとjに含まれる名詞句はそれぞれ，XとYに置換する\n",
    "> \n",
    "> また，係り受けパスの形状は，以下の2通りが考えられる．\n",
    "> * 文節iから構文木の根に至る経路上に文節jが存在する場合: 文節iから文節jのパスを表示\n",
    "> * 上記以外で，文節iと文節jから構文木の根に至る経路上で共通の文節kで交わる場合: 文節iから文節kに至る直前のパスと文節jから文節kに至る直前までのパス，文節kの内容を” | “で連結して表示\n",
    "> \n",
    "> 「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える． CaboChaを係り受け解析に用いた場合，次のような出力が得られると思われる．\n",
    "> >Xは | Yに関する -> 最初の -> 会議で | 作り出した  \n",
    "> >Xは | Yの -> 会議で | 作り出した  \n",
    "> >Xは | Yで | 作り出した  \n",
    "> >Xは | Yという -> 用語を | 作り出した  \n",
    "> >Xは | Yを | 作り出した  \n",
    "> >Xに関する -> Yの  \n",
    "> >Xに関する -> 最初の -> Yで  \n",
    "> >Xに関する -> 最初の -> 会議で | Yという -> 用語を | 作り出した  \n",
    "> >Xに関する -> 最初の -> 会議で | Yを | 作り出した  \n",
    "> >Xの -> Yで  \n",
    "> >Xの -> 会議で | Yという -> 用語を | 作り出した  \n",
    "> >Xの -> 会議で | Yを | 作り出した  \n",
    "> >Xで | Yという -> 用語を | 作り出した  \n",
    "> >Xで | Yを | 作り出した  \n",
    "> >Xという -> Yを\n",
    "> \n",
    "> KNPを係り受け解析に用いた場合，次のような出力が得られると思われる．\n",
    "> >Xは | Yに -> 関する -> 会議で | 作り出した。  \n",
    "> >Xは | Yで | 作り出した。  \n",
    "> >Xは | Yと -> いう -> 用語を | 作り出した。  \n",
    "> >Xは | Yを | 作り出した。  \n",
    "> >Xに -> 関する -> Yで  \n",
    "> >Xに -> 関する -> 会議で | Yと -> いう -> 用語を | 作り出した。  \n",
    "> >Xに -> 関する -> 会議で | Yを | 作り出した。  \n",
    "> >Xで | Yと -> いう -> 用語を | 作り出した。  \n",
    "> >Xで | Yを | 作り出した。  \n",
    "> >Xと -> いう -> Yを\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A0dB7Mk_2IIk"
   },
   "source": [
    "### コメント\n",
    "\n",
    "- いまいちルールがわかっていないのでパス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOf9MBP8oV5w9glSXOQ7/Fv",
   "collapsed_sections": [],
   "name": "100knocks_chapter5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
