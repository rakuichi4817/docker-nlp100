{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e2afadb-2bbf-41f7-b829-f10c3c086c06",
   "metadata": {},
   "source": [
    "# 第8章：ニューラルネット\n",
    "\n",
    "> 第6章で取り組んだニュース記事のカテゴリ分類を題材として，ニューラルネットワークでカテゴリ分類モデルを実装する．なお，この章ではPyTorch, TensorFlow, Chainerなどの機械学習プラットフォームを活用せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbee91e5-7931-404e-8af4-088646ffdd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "import shutil\n",
    "import string\n",
    "import random\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c6f7e3-62bc-48bf-98bd-f1d3b5f1f7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = \"data\"          # データを保存するおおもとのディレクトリ\n",
    "CURRENTDIR = \"/workspace/notebook\" # notebookディレクトリへのパス\n",
    "CHPDIR = os.path.join(DATADIR, \"chapter8\")\n",
    "CHP6DIR = os.path.join(DATADIR, \"chapter6\")\n",
    "CHP7DIR = os.path.join(DATADIR, \"chapter7\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(CHPDIR)\n",
    "except:\n",
    "    print(\"作成済み等の理由でディレクトリが作成されませんでした\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95d88dc-ff6e-4b0a-a6f7-fcdcd9d7c1ad",
   "metadata": {},
   "source": [
    "## 70. 単語ベクトルの和による特徴量\n",
    "\n",
    "問題50で構築したデータをベクトルに変換する。\\\n",
    "文章の単語をすべて単語ベクトルに置き換えたのち、平均をとる形で実装する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382a3c58-7360-4a13-a0d4-fa1f207b2e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用するデータをchapter6のディレクトリから読み込む\n",
    "fnames = [\"train.txt\", \"valid.txt\", \"test.txt\"]\n",
    "chp6dir = os.path.join(DATADIR, \"chapter6\")\n",
    "\n",
    "all_data = {}\n",
    "\n",
    "for fname in fnames:\n",
    "    temp_fpath = os.path.join(CHP6DIR, fname)\n",
    "    with open(temp_fpath, \"r\", encoding=\"utf8\")as fr:\n",
    "        # タブ区切りをリストで取得\n",
    "        data = [line.rstrip(\"\\n\").split(\"\\t\") for line in fr]\n",
    "        all_data[fname] = data\n",
    "\n",
    "# 各カテゴリの事例数を確認する\n",
    "for fname, data in all_data.items():\n",
    "    print(\"【{}】\".format(fname))\n",
    "    pprint(collections.Counter([record[0] for record in data]))\n",
    "\n",
    "# ラベルベクトルの作成\n",
    "categorys = {\"b\": 0, \"t\": 1, \"e\":2, \"m\":3}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2397c5a2-7c6c-4472-b1c0-bb813a0b5237",
   "metadata": {},
   "source": [
    "### ------------↓以下の作業は初回実行時のみ行う↓------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03d1086-9c92-4ecb-aff1-61e125263b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vecモデルを読み込む\n",
    "# モデルの読み込み（割と時間かかる）\n",
    "if input(\"実行しますか？（y/n）2回目以降はpickleを読み込むセルから実行\"):\n",
    "    model_path = os.path.join(CHP7DIR, \"GoogleNews-vectors-negative300.bin.gz\")\n",
    "    model = KeyedVectors.load_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c55e94-a5c7-4265-bad2-82b07470c200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文書→ベクトル関数の作成\n",
    "def transform_d2v(text):\n",
    "    table = str.maketrans(string.punctuation, \" \"*len(string.punctuation))\n",
    "    words = text.translate(table).split()  # 記号をスペースに置換後、スペースで分割してリスト化\n",
    "    vec = [model[word] for word in words if word in model]  # 1語ずつベクトル化\n",
    "    \n",
    "    return torch.tensor(sum(vec) / len(vec))  # 平均ベクトルをTensor型に変換して出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b7a3ad-c120-494e-b640-b2d47f7f46d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴ベクトル化実行\n",
    "X_train = torch.stack([transform_d2v(doc_data[1]) for doc_data in all_data[\"train.txt\"]])\n",
    "X_valid = torch.stack([transform_d2v(doc_data[1]) for doc_data in all_data[\"valid.txt\"]])\n",
    "X_test = torch.stack([transform_d2v(doc_data[1]) for doc_data in all_data[\"test.txt\"]])\n",
    "\n",
    "print(X_train.shape, X_valid.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59262c1b-b3a2-4034-8604-9ec6c9cc4977",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.tensor([categorys[doc_data[0]] for doc_data in all_data[\"train.txt\"]])\n",
    "y_valid = torch.tensor([categorys[doc_data[0]] for doc_data in all_data[\"valid.txt\"]])\n",
    "y_test = torch.tensor([categorys[doc_data[0]] for doc_data in all_data[\"test.txt\"]])\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dc9dfd-3e4a-42b3-a3e8-d52bef2226ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存\n",
    "pt_files = [\"X_train.pt\", \"X_valid.pt\", \"X_test.pt\", \"y_train.pt\", \"y_valid.pt\", \"y_test.pt\"]\n",
    "save_objs = [X_train, X_valid, X_test, y_train, y_valid, y_test]\n",
    "\n",
    "for pt_file, save_obj in zip(pt_files, save_objs):\n",
    "    output_path = os.path.join(CHPDIR, pt_file)\n",
    "    torch.save(save_obj, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ddfd3d-cb0f-4b71-96c6-fea095ba89e0",
   "metadata": {},
   "source": [
    "### ------------↑ここまで↑------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529b6f78-b0f7-4683-8177-b1b41f22e84e",
   "metadata": {},
   "source": [
    "### ------------↓2回目以降の作業↓------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a21c4e2-c9ef-4ffa-b650-f051817e7bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各ベクトルのロード\n",
    "X_train = torch.load(os.path.join(CHPDIR, \"X_train.pt\"))\n",
    "X_valid = torch.load(os.path.join(CHPDIR, \"X_valid.pt\"))\n",
    "X_test = torch.load(os.path.join(CHPDIR, \"X_test.pt\"))\n",
    "y_train = torch.load(os.path.join(CHPDIR, \"y_train.pt\"))\n",
    "y_valid = torch.load(os.path.join(CHPDIR, \"y_valid.pt\"))\n",
    "y_test = torch.load(os.path.join(CHPDIR, \"y_test.pt\"))\n",
    "\n",
    "# 確認\n",
    "print(X_train.shape, X_valid.shape, X_test.shape)\n",
    "print(y_train.shape, y_valid.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87515bce-de95-4f27-ac46-18158b7dc072",
   "metadata": {},
   "source": [
    "## 71. 単層ニューラルネットワークによる予測\n",
    "\n",
    "70で作成したベクトルについてソフトマックスによる計算を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef92dda3-d2c4-4e82-af2d-a01d34acb697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# レイヤーの定義\n",
    "class SLPNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(SLPNet, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size, bias=False)\n",
    "        nn.init.normal_(self.fc.weight, 0.0, 1.0)  # 正規乱数で重みを初期化\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2735106-eb1f-4ed0-af3d-7dcd398fd0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1行分投入\n",
    "model = SLPNet(300, 4)  # 単層ニューラルネットワークの初期化\n",
    "y_hat_1 = torch.softmax(model(X_train[:1]), dim=-1)\n",
    "print(y_hat_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c32f976-35de-4aee-ab6c-3199b75094ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4行分投入\n",
    "Y_hat = torch.softmax(model.forward(X_train[:4]), dim=-1)\n",
    "print(Y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26a4feb-d819-46e3-9356-82622511684e",
   "metadata": {},
   "source": [
    "## 72. 損失と勾配の計算\n",
    "\n",
    "クロスエントロピー損失と行列Wに対する勾配を求める"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0273c6b9-f9d9-4dab-b9e1-28b1f5b1778c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交差エントロピー誤差の用意\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# ソフトマックスだけ行ったものをラベルと比較\n",
    "l_1 = criterion(model(X_train[:1]), y_train[:1])\n",
    "# 勾配の初期化\n",
    "model.zero_grad()\n",
    "# 勾配の計算\n",
    "l_1.backward()\n",
    "print(f\"損失:{l_1:.4f}\")\n",
    "print(f\"勾配:{model.fc.weight.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83c0b67-e051-4e30-b6b3-22b9b7e1c612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ソフトマックスだけ行ったものをラベルと比較\n",
    "l_1 = criterion(model(X_train[:4]), y_train[:4])\n",
    "# 勾配の初期化\n",
    "model.zero_grad()\n",
    "# 勾配の計算\n",
    "l_1.backward()\n",
    "print(f\"損失:{l_1:.4f}\")\n",
    "print(f\"勾配:{model.fc.weight.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c671ed-bece-46cb-964d-64f48ba1ef37",
   "metadata": {},
   "source": [
    "## 73. 確率的勾配降下法による学習\n",
    "\n",
    "> 確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，行列WWを学習せよ．なお，学習は適当な基準で終了させればよい（例えば「100エポックで終了」など）．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5664bb8-7f6a-48d4-a2cb-3f587afb3c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.y[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021ce31e-3e46-4d0b-89e7-c88cb2ee7486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasetの作成\n",
    "dataset_train = NewsDataset(X_train, y_train)\n",
    "dataset_valid = NewsDataset(X_valid, y_valid)\n",
    "dataset_test = NewsDataset(X_test, y_test)\n",
    "\n",
    "# DataLoaderの作成\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=1, shuffle=True)\n",
    "dataloader_valid = DataLoader(dataset_valid, batch_size=len(dataset_valid), shuffle=False)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=len(dataset_test), shuffle=False)\n",
    "\n",
    "dataloader_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba196b4-e00f-4afc-88fb-f7419364f2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの定義\n",
    "model = SLPNet(300, 4)\n",
    "\n",
    "# 損失関数の定義\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# オプティマイザの定義（確率的勾配降下法）\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
    "\n",
    "# 学習\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # 訓練モードにする\n",
    "    model.train()\n",
    "    loss_train = 0.0\n",
    "    for i, (inputs, labels) in enumerate(dataloader_train):\n",
    "        # 勾配を0で初期化\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 順伝播 + 誤差逆伝播 + 重み更新\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 損失を記憶\n",
    "        loss_train += loss.item()\n",
    "        \n",
    "    # バッチ単位の平均損失計算\n",
    "    loss_train = loss_train / i\n",
    "    \n",
    "    # 検証データの損失計算\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs, labels = next(iter(dataloader_valid))\n",
    "        outputs = model(inputs)\n",
    "        loss_valid = criterion(outputs, labels)\n",
    "        \n",
    "    print(f\"epoch: {epoch + 1}, loss_train: {loss_train:.4f}, loss_valid: {loss_valid:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5202e061-5689-4198-a01b-371aa32bda1f",
   "metadata": {},
   "source": [
    "## 74. 正解率の計測\n",
    "\n",
    "> 問題73で求めた行列を用いて学習データおよび評価データの事例を分類したとき，その正解率をそれぞれ求めよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad7dfb6-1766-40bb-8bae-347407817b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, loader):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            outputs = model(inputs)\n",
    "            pred = torch.argmax(outputs, dim=-1)\n",
    "            total += len(inputs)\n",
    "            correct += (pred == labels).sum().item()\n",
    "            \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8f265e-caf5-4b58-993f-2c2d966e31d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train = get_accuracy(model, dataloader_train)\n",
    "acc_test = get_accuracy(model, dataloader_test)\n",
    "\n",
    "print(f\"正解率（学習データ）：{acc_train:.3f}\")\n",
    "print(f\"正解率（評価データ）：{acc_test:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417beb54-90a4-47cb-83d1-3a8e0348f9f9",
   "metadata": {},
   "source": [
    "## 75. 損失と正解率のプロット\n",
    "\n",
    "> 問題73のコードを改変し，各エポックのパラメータ更新が完了するたびに，訓練データでの損失，正解率，検証データでの損失，正解率をグラフにプロットし，学習の進捗状況を確認できるようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d507b0-ae13-445e-9a14-f09f3cc84a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_and_accuracy(model, criterion, loader):\n",
    "    model.eval()\n",
    "    loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            outputs = model(inputs)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            pred = torch.argmax(outputs, dim=-1)\n",
    "            total += len(inputs)\n",
    "            correct += (pred == labels).sum().item()\n",
    "    \n",
    "    return loss / len(loader), correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70b8b5b-0638-423e-8236-b3d78a188a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの定義\n",
    "model = SLPNet(300, 4)\n",
    "\n",
    "# 損失関数の定義\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# オプティマイザの定義（確率的勾配降下法）\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
    "\n",
    "# 学習\n",
    "num_epochs = 30\n",
    "log_train = []\n",
    "log_valid = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 訓練モードにする\n",
    "    model.train()\n",
    "    for i, (inputs, labels) in enumerate(dataloader_train):\n",
    "        # 勾配を0で初期化\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 順伝播 + 誤差逆伝播 + 重み更新\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # 損失と正解率の算出\n",
    "    loss_train, acc_train = get_loss_and_accuracy(model, criterion, dataloader_train)\n",
    "    loss_valid, acc_valid = get_loss_and_accuracy(model, criterion, dataloader_valid)\n",
    "    log_train.append([loss_train, acc_train])\n",
    "    log_valid.append([loss_valid, acc_valid])\n",
    "    \n",
    "    # ログを出力\n",
    "    print(f\"epoch: {epoch + 1}, loss_train: {loss_train:.4f}, accuracy_train: {acc_train:.4f}, loss_valid: {loss_valid:.4f}, accuracy_valid: {acc_valid:.4f}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f02f36-4fae-4421-81ee-d380a1eebfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覚化\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax[0].plot(np.array(log_train).T[0], label=\"train\")\n",
    "ax[0].plot(np.array(log_valid).T[0], label=\"valid\")\n",
    "ax[0].set_xlabel(\"epoch\")\n",
    "ax[0].set_ylabel(\"loss\")\n",
    "ax[0].legend()\n",
    "ax[1].plot(np.array(log_train).T[1], label=\"train\")\n",
    "ax[1].plot(np.array(log_valid).T[1], label=\"valid\")\n",
    "ax[1].set_xlabel(\"epoch\")\n",
    "ax[1].set_ylabel(\"accuracy\")\n",
    "ax[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92d0d0b-f02f-415f-8e4d-cf8c580d00f7",
   "metadata": {},
   "source": [
    "## 76. チェックポイント\n",
    "\n",
    "> 問題75のコードを改変し，各エポックのパラメータ更新が完了するたびに，チェックポイント（学習途中のパラメータ（重み行列など）の値や最適化アルゴリズムの内部状態）をファイルに書き出せ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bc705e-aab0-4446-a473-a75db8591a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの定義\n",
    "model = SLPNet(300, 4)\n",
    "\n",
    "# 損失関数の定義\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# オプティマイザの定義\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
    "\n",
    "# 学習\n",
    "num_epochs = 10\n",
    "log_train = []\n",
    "log_valid = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 訓練モードにする\n",
    "    model.train()\n",
    "    for i, (inputs, labels) in enumerate(dataloader_train):\n",
    "        # 勾配を0で初期化\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 順伝播 + 誤差逆伝播 + 重み更新\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # 損失と正解率の算出\n",
    "    loss_train, acc_train = get_loss_and_accuracy(model, criterion, dataloader_train)\n",
    "    loss_valid, acc_valid = get_loss_and_accuracy(model, criterion, dataloader_valid)\n",
    "    log_train.append([loss_train, acc_train])\n",
    "    log_valid.append([loss_valid, acc_valid])\n",
    "    \n",
    "    checkpoint_path = os.path.join(CHPDIR, f\"checkpoint{epoch + 1}.pt\")\n",
    "    \n",
    "    # チェックポイントの保存\n",
    "    torch.save({\"epoch\": epoch, \"model_state_dict\": model.state_dict(), \n",
    "                F\"optimizer_state_dict\": optimizer.state_dict()}, checkpoint_path)\n",
    "    \n",
    "    # ログを出力\n",
    "    print(f\"epoch: {epoch + 1}, loss_train: {loss_train:.4f}, accuracy_train: {acc_train:.4f}, loss_valid: {loss_valid:.4f}, accuracy_valid: {acc_valid:.4f}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaa55be-ed72-4de8-89a9-a7bb1e5e6b0b",
   "metadata": {},
   "source": [
    "## 77. ミニバッチ化\n",
    "\n",
    "> 問題76のコードを改変し，B事例ごとに損失・勾配を計算し，行列Wの値を更新せよ（ミニバッチ化）．Bの値を1,2,4,8,…と変化させながら，1エポックの学習に要する時間を比較せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ff0e75-de86-4a3f-bd45-401fb8b0900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "        dataset_train, dataset_valid, batch_size, model, \n",
    "        criterion, optimizer, num_epochs):\n",
    "    # dataloaderの作成\n",
    "    dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "    dataloader_valid = DataLoader(dataset_valid, batch_size=len(dataset_valid), shuffle=False)\n",
    "    \n",
    "    # 学習\n",
    "    log_train = []\n",
    "    log_valid = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # 開始時刻の記録\n",
    "        s_time = time.time()\n",
    "        \n",
    "        # 訓練モードにする\n",
    "        model.train()\n",
    "        for i, (inputs, labels) in enumerate(dataloader_train):\n",
    "            # 勾配を0で初期化\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 順伝播 + 誤差逆伝播 + 重み更新\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # 損失と正解率の算出\n",
    "        loss_train, acc_train = get_loss_and_accuracy(model, criterion, dataloader_train)\n",
    "        loss_valid, acc_valid = get_loss_and_accuracy(model, criterion, dataloader_valid)\n",
    "        log_train.append([loss_train, acc_train])\n",
    "        log_valid.append([loss_valid, acc_valid])\n",
    "\n",
    "        checkpoint_path = os.path.join(CHPDIR, f\"checkpoint{epoch + 1}.pt\")\n",
    "\n",
    "        # チェックポイントの保存\n",
    "        torch.save({\"epoch\": epoch, \"model_state_dict\": model.state_dict(), \n",
    "                    F\"optimizer_state_dict\": optimizer.state_dict()}, checkpoint_path)\n",
    "\n",
    "        # 終了時刻の記録\n",
    "        e_time = time.time()\n",
    "        \n",
    "        # ログを出力\n",
    "        print(f\"epoch: {epoch + 1}, loss_train: {loss_train:.4f}, accuracy_train: {acc_train:.4f}, loss_valid: {loss_valid:.4f},\\\n",
    "                accuracy_valid: {acc_valid:.4f}, {(e_time - s_time):.4f}sec\")  \n",
    "    \n",
    "    return {\"train\": log_train, \"valid\": log_valid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aad217c-f793-47b9-a96d-ad90006456aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasetの作成\n",
    "dataset_train = NewsDataset(X_train, y_train)\n",
    "dataset_valid = NewsDataset(X_valid, y_valid)\n",
    "\n",
    "# モデルの定義\n",
    "model = SLPNet(300, 4)\n",
    "\n",
    "# 損失関数の定義\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# オプティマイザの定義\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
    "\n",
    "# モデルの学習\n",
    "for batch_size in [2 ** i for i in range(11)]:\n",
    "    print(f\"バッチサイズ: {batch_size}\")\n",
    "    log = train_model(dataset_train, dataset_valid, batch_size, model, criterion, optimizer, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f08e493-b30a-40f7-a4c0-133038020236",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
